---
title: "Case study and typical operations with base R"
author: "Advanced R"
date: "Wednesday May 2, 2018"
output: html_document
---

```{r setup, include=FALSE}
# knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(message=FALSE, warning=FALSE)
```


## Learning goals

1. Introduce mass spectrometry data and common data analysis tasks for the case study.

2. Review tools in base R for the tasks.


## A typical bottom-up MS-based proteomics experiment

* Design: to compare **protein** abundance between **groups** of interest (e.g., healthy vs. diseased subjects). 

* Measurements: **feature intensities** measured by mass spectrometry
    - **Proteins** are cleaved into peptides.
    - **Peptides** are charged, fragmented and measured by mass spectrometry.
    - **Features** (fragments of charged peptides) are the basic unit of quantification.


```{r, warning=FALSE, message=FALSE}
load("adv-R-twin.RData")
```

```{r, fig.width=8, fig.height=5, echo=FALSE, fig.align='center', warning=FALSE, message=FALSE}
library(tidyverse)
sub <- as_tibble(twin_dia) %>% 
    filter(grepl(paste(sprintf("%03d", 1:20), collapse = "|"), run)) %>% 
    rename(heavy = intensity_h, light = intensity_l) %>% 
    gather(label, intensity, heavy, light)

sub %>% 
    filter(protein == "APOA") %>% 
    ggplot(aes(run, log2(intensity), group = feature, colour = feature)) + 
    geom_point() + 
    geom_line() + 
    ggtitle("APOA") + 
    facet_wrap(~ label) + 
    theme(axis.text.x = element_text(angle = 90, hjust = 1)) + 
    theme(legend.position = "bottom")

sub %>% 
    ggplot(aes(run, log2(intensity))) + geom_boxplot() + 
    facet_wrap(~ label) + 
    theme(axis.text.x = element_text(angle = 90, hjust = 1))
```


## Case study

Liu et al., Quantitative variability of 342 plasma proteins in a human twin population, *Molecular Systems Biology*, 2015. [PMID: 25652787]

* The dataset contains `r length(unique(twin_dia$run))` MS runs of plasma samples:
    - `r length(unique(twin_dia$pair))` pairs of monozygotic (MZ) and dizygotic (DZ) twins
    - `r length(unique(twin_dia$visit))` time points
    - $58 \times 2 \times 2 = 232$

* Data were acquired with two MS workflows:
    - data independent acquisition (DIA)
    - selected reaction monitoring (SRM)

* We use a subset of the original dataset (with `r length(unique(twin_dia$protein))` proteins by DIA and `r length(unique(twin_srm$protein))` proteins by SRM).

```{r}
str(twin_dia)
```

The two data frames have the same format, containing 9 columns:

* `protein` (chr): protein name
* `feature` (chr): combination of peptide, precursor charge state, fragment ion, and product charge state, separated by `_`
* `run` (chr): MS run identifier (R001-R232)
* `pair` (int): pair identifier number (1-58)
* `zygosity` (factor): zygosity (MZ, DZ)
* `subject` (int): subject identifier number (1-116)
* `visit` (int): time of visit (1, 2)
* `intensity_l` (num): integrated feature intensity from light (L) channel
* `intensity_h` (num): integrated feature intensity from heavy (H, aka reference) channel

```{r}
head(twin_dia)
```

```{r}
class(twin_dia)
```

A **data frame** contains **variables** (in **columns**) and **observations** (in **rows**) in a tabular form. It is essentially a **list** of vectors of equal length. The columns in a data frame have the same length, but they may represent different types of variables (e.g., numeric, character, logical, etc.). Data frames are preferable in most analysis tasks because:

* Data frame keeps together related values of variables (in a row).
* Most functions for statistical modeling, inference and graphing in R take data frame as a primary input format, passed through a `data = ` argument. 


Dimension of the data:

```{r, eval=F}
dim(twin_dia)
nrow(twin_dia)
ncol(twin_dia)
```

Level of the categorical variable:

```{r, eval=F}
levels(twin_dia$zygosity)
```

`View()` calls the data viewer in RStudio.

```{r, eval=FALSE}
View(twin_dia)
```


## Data analysis tasks for the case study

* **Task 1:** compute the median of log-intensities for features from the heavy channel in each run.
* **Task 2:** normalization of feature intensities to remove systematic bias across runs.
* **Task 3:** summarization of feature intensities for each protein in every run.
* **Task 4:** fit a linear model to characterize the summarized intensities.
* **Task 5:** group comparison using model-based inference.

While R offers several tools that can be applied for each task, it is important to note how they can (or cannot) be integrated into a consistent workflow.


## Task 1: median of log-intensities (heavy channel) per run

This is a very common data analysis task: making grouped summaries, which involves application of the **split-apply-combine** approach. Let's begin with a review of necessary tools for data manipulation and transformation.


### Data manipulation and transformation

Three common operations: 

* Extract existing variables (with `$`, column names, or indices). 
* Extract existing observations (with logical operations on the variables). 
* Add new variables. 


#### Extract existing variables

Keep only a subset of variables relevant to the analysis. For example, to create a look-up table for sample annotation: 

```{r}
colnames(twin_dia)

# Extract columns for sample annotation
design <- unique(twin_dia[, c("run", "pair", "zygosity", "subject", "visit")])
head(design)
```


#### Extract existing observations with logical operations

Extract observations based on values in specific columns:

```{r}
# Exclude rows with NA in column intensity_h
sub_dia <- twin_dia[!is.na(twin_dia$intensity_h), ]
head(sub_dia)
```

Combine multiple logical operations:

* element-wise AND operation with `&`
* element-wise OR operation with `|` 

```{r}
# Filter based on columns intensity_h and run
sub_dia <- twin_dia[!is.na(twin_dia$intensity_h) & twin_dia$run == "R001", ]
head(sub_dia)
```


#### Add new variables

```{r}
# Log2 transformation
twin_dia$log2inty_h <- log2(twin_dia$intensity_h)
twin_dia$log2inty_l <- log2(twin_dia$intensity_l)
```


#### Combine multiple operatons 

Compute the median of feature intensities from the heavy channel (`log2inty_h`) in one run (`R001`):

```{r}
median(twin_dia$log2inty_h[!is.na(twin_dia$log2inty_h) & twin_dia$run == "R001"])
```

There are two issues when combining multiple operations with functions in base R: 

1. The name of the data frame is repeated several times. This can be avoided by using `with()`, as in 
`with(twin_dia, median(log2inty_h[!is.na(log2inty_h) & run == "R001"]))`.

2. The **nested representation** makes the flow of operation less readable and thus error-prone. Creating intermediate objects may be helpful if the objects are named properly (naming itself can be hard though).


### The split-apply-combine approach

A very common pattern to make grouped summaries: 

* **Split** a vector `X` into subsets defined by a group-indication vector `GROUP`.
* **Apply** function `FUN` to each subset.
* **Combine** the results and return in a convenient form.

In Task 1, to compute the medians of log-intensities in all runs, we need to split the column vector `log2inty_h` into subsets defined by `run`, apply `median()` to each subset, and combine the results.


### Approach 1a: use a `for` loop

```{r}
# Approach 1a
runs <- unique(twin_dia$run)

medians <- rep(0, length(runs))  # create a vector to restore the result
for (i in seq_along(runs)) {
    medians[i] <- median(twin_dia$log2inty_h[twin_dia$run == runs[i]], na.rm = TRUE)
}
str(medians)
```


### Approach 1b: use `tapply()`

Syntax: `tapply(X, GROUP, FUN)`.

```{r}
# Approach 1b
medians <- tapply(twin_dia$log2inty_h, twin_dia$run, median, na.rm = TRUE)
head(medians)  # a named vector is returned
```


### Approach 1c: use `aggregate()`

Syntax: `aggregate(X, GROUP, FUN)`.

```{r}
# Approach 1c
df_median <- aggregate(twin_dia$log2inty_h, list(run = twin_dia$run), median, na.rm = TRUE)
head(df_median)  # a data frame is returned
```

```{r}
# Rename second column for later use
colnames(df_median)[2] <- "run_median"
```

`tapply()` and `aggregate()` are tools for **functional programming** that will be discussed in greater detail in a later session. Note that their outputs are of different classes.


## Task 2: normalization

Adjust the log-intensities to equalize the medians across runs to a global median: 

```{r}
(gbl_median <- median(medians, na.rm = TRUE))
```


### Approach 2a: use a `for` loop

```{r}
# Use a for loop
for (ii in names(medians)) {
    log2inty_h <- twin_dia$log2inty_h[twin_dia$run == ii]
    log2inty_l <- twin_dia$log2inty_l[twin_dia$run == ii]
    twin_dia$log2inty_h[twin_dia$run == ii] <- log2inty_h - medians[ii] + gbl_median
    twin_dia$log2inty_l[twin_dia$run == ii] <- log2inty_l - medians[ii] + gbl_median
}
```

```{r}
# Check the normalized medians
head(tapply(twin_dia$log2inty_h, twin_dia$run, median, na.rm = TRUE))

# Back to unnormalized values for Approach 2b
twin_dia$log2inty_h <- log2(twin_dia$intensity_h)
twin_dia$log2inty_l <- log2(twin_dia$intensity_l)
```


### Approach 2b: use a vectorized representation 

A more efficient way is to use a vectorized representation with the named vector returned by `tapply()`: 

```{r}
# This gives the medians of runs R001, R002, R003, R001
medians[c("R001", "R002", "R003", "R001")]
```

```{r}
# Use vectorized representation to normalize the dataset
twin_dia$log2inty_h <- twin_dia$log2inty_h - medians[twin_dia$run] + gbl_median
twin_dia$log2inty_l <- twin_dia$log2inty_l - medians[twin_dia$run] + gbl_median
```

```{r}
# Check the normalized medians
head(tapply(twin_dia$log2inty_h, twin_dia$run, median, na.rm = TRUE))

# Back to unnormalized values for Approach 2c
twin_dia$log2inty_h <- log2(twin_dia$intensity_h)
twin_dia$log2inty_l <- log2(twin_dia$intensity_l)
```


### Approach 2c: merge computed medians to original data frame

We can also add one column for the median in each run (from `df_median` returned by `aggregate()`) to the original data frame `twin_dia`, with `merge()`. 

The operation `merge(X, Y, by.x = "X_COL", by.y = "Y_COL")` joins two data frames `X`, `Y`, by matching the columns `X_COL` and `Y_COL`.

* Default (when `by.x` and `by.y` are not specified) is to match all columns with common names.
* It returns a new data frame that has all the columns in `X` and `Y`.

```{r}
twin_dia2 <- merge(x = twin_dia, y = df_median)
head(twin_dia2)

twin_dia2$log2inty_h <- twin_dia2$log2inty_h - twin_dia2$run_median + gbl_median
twin_dia2$log2inty_l <- twin_dia2$log2inty_l - twin_dia2$run_median + gbl_median

head(tapply(twin_dia2$log2inty_h, twin_dia2$run, median, na.rm = TRUE))
```


## Task 3: summarization of feature intensities

For each **protein** and each **run**, compute the **log of the sum** of the normalized feature intensities in the light channel.

```{r}
# Transform the normalized log-intensities back to the original scale
twin_dia2$intensity_h <- 2 ^ (twin_dia2$log2inty_h)
twin_dia2$intensity_l <- 2 ^ (twin_dia2$log2inty_l)
```


### Approach 3a: use two `for` loops

One `for` loop for protein and the other for run. Skipped here.


### Approach 3b: use `tapply()`

Use `tapply()` with grouping variables defined in a **list**:

```{r}
# Approach 3b
sum_t <- tapply(
    twin_dia2$intensity_l, 
    list(run = twin_dia2$run, protein = twin_dia2$protein), 
    function(x) log2(sum(x, na.rm = TRUE))
)
head(sum_t)
```


### Approach 3c: use `aggregate()`

```{r}
# Approach 3c
sum_g <- aggregate(
    twin_dia2$intensity_l,
    list(run = twin_dia2$run, protein = twin_dia2$protein),
    function(x) log2(sum(x, na.rm = TRUE))
)
head(sum_g)
```

```{r, eval=FALSE}
# Alternatively, use a formula representation
sum_g <- aggregate(
    intensity_l ~ run + protein, 
    data = twin_dia2, 
    function(x) log2(sum(x, na.rm = TRUE))
)
```

```{r}
# Rename the third column
colnames(sum_g)[3] <- "log2inty"
```

Saving the grouped summaries in a data frame makes it easier to carry out statistical modeling and inference in subsequent analysis.


## Task 4: fit a linear model to characterize summarized intensities

For each **protein**, fit a linear model, extract summaries of the fitted model, and draw model-based inference.


### Fit a linear regression model with `lm()`

The first argument of `lm()` is a formula, e.g., `Y ~ X1 + X2`, where `Y` is the response and `X1`, `X2` are the predictive variables. These refer to the column names (variables) in a data frame, that we pass on to `lm()` through the `data` argument.

Suppose we are interested in knowing if the abundance of one **protein** is associated with **zygosity**.

```{r}
# Merge the design information
df_sum <- merge(sum_g, design)
head(df_sum)

sub_sum <- df_sum[df_sum$protein == "A2MG", ]  # Subset for protein A2MG
fit <- lm(log2inty ~ zygosity, data = sub_sum)
```

```{r, eval=FALSE}
# Same as
fit <- lm(log2inty ~ zygosity, data = df_sum, subset = df_sum$protein == "A2MG")
```

```{r}
class(fit)
```


### Utility functions

R offers a couple of utility functions for linear models, such as `coef()`, `fitted()`, `residuals()`, `summary()`, `predict()`, for retrieving coefficients, fitted values, residuals, model summary, making predictions with the model, respectively.

These functions can be applied in the same way for other model objects such as `glm()`, `gam()`, etc.


#### Use `summary()` to display an overview of the fitted model

```{r}
summary(fit)
```

```{r, eval=FALSE}
# More detail
str(summary(fit))
```


#### Use `coef()` to retrieve estimated coefficients

```{r}
coef(fit)
```


#### Use `fitted()` to retrieve fitted values

```{r}
head(fitted(fit))
```

To derive summaries for all the proteins, additional efforts are required to extract relevant information and package them into a convenient format. If we want to avoid using `for` loops, we will need to define specialized functions to extract summaries of interest that can be passed on to `tapply()` or `aggregate()`.


## Task 5: model-based inference

Two-sample $t$-test on the subset of summaries for protein A2MG:

```{r}
ttest <- t.test(log2inty ~ zygosity, data = sub_sum)
ttest
```

It's equivalent to use the `subset` option:

```{r, eval=FALSE}
t.test(log2inty ~ zygosity, data = df_sum, subset = df_sum$protein == "A2MG")
```

Alternatively, you can use two vectors for the two samples:

```{r, eval=FALSE}
t.test(x = sub_sum$log2inty[sub_sum$zygosity == "DZ"], y = sub_sum$log2inty[sub_sum$zygosity == "MZ"])
```

Take a look at the output object:

```{r}
str(ttest)
```

```{r}
ttest$estimate
ttest$statistic
ttest$p.value
```

As in Task 4, it requires additional efforts to work with model objects, in order to make and combine summaries for all the proteins: 

* How would you implement a workflow to summarize the means of two groups, difference, $t$-statistic, $p$-value for every protein? 
* Do you see any inconvenience or inconsistency in the workflow? If so, where do they come from?
